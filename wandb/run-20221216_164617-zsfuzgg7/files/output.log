loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Number of training examples: 111
Number of validation examples: 14
Some weights of DeformableDetrForObjectDetection were not initialized from the model checkpoint at SenseTime/deformable-detr and are newly initialized because the shapes did not match:
- class_embed.0.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated
- class_embed.0.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([1]) in the model instantiated
- class_embed.1.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated
- class_embed.1.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([1]) in the model instantiated
- class_embed.2.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated
- class_embed.2.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([1]) in the model instantiated
- class_embed.3.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated
- class_embed.3.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([1]) in the model instantiated
- class_embed.4.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated
- class_embed.4.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([1]) in the model instantiated
- class_embed.5.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated
- class_embed.5.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([1]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. Trainer(distributed_backend=dp) (or ddp, ddp2). Setting distributed_backend=ddp_spawn for you.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.
  warnings.warn(*args, **kwargs)
Traceback (most recent call last):
  File "fine_tune_d-detr2.py", line 173, in <module>
    main()
  File "fine_tune_d-detr2.py", line 125, in main
    trainer.fit(model)
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 988, in fit
    results = self.__run_ddp_spawn(model, nprocs=self.num_processes)
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1068, in __run_ddp_spawn
    mp.spawn(self.ddp_train, nprocs=nprocs, args=(q, model, ))
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException:
-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 535, in ddp_train
    model.cuda(self.root_gpu)
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/pytorch_lightning/utilities/device_dtype_mixin.py", line 110, in cuda
    return super().cuda(device=device)
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/torch/nn/modules/module.py", line 747, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/torch/nn/modules/module.py", line 639, in _apply
    module._apply(fn)
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/torch/nn/modules/module.py", line 639, in _apply
    module._apply(fn)
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/torch/nn/modules/module.py", line 639, in _apply
    module._apply(fn)
  [Previous line repeated 6 more times]
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/torch/nn/modules/module.py", line 662, in _apply
    param_applied = fn(param)
  File "/scratch/staff/jrs596/python_environments/envs/def_detr_pl/lib/python3.7/site-packages/torch/nn/modules/module.py", line 747, in <lambda>
    return self._apply(lambda t: t.cuda(device))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.92 GiB total capacity; 1.83 MiB already allocated; 4.25 MiB free; 2.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF